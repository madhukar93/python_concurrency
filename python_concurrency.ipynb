{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Python concurrency story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes\n",
    "- Let's begin with what concurrency means\n",
    "- Web search led me to this blog with the same name, I'll leave a link there. Check it out, it has useful benchmarks etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency\n",
    "\n",
    "[dictionary.com](http://www.dictionary.com/browse/concurrent)\n",
    "- occurring or existing simultaneously or side by side\n",
    "- acting in conjunction; cooperating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes\n",
    "- Ok, that's the literal dictionary definition, but why is it relevant for us as people writing software, and why does the world seem to care suddenly ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiters and tables\n",
    "- 5 tables\n",
    "- 1 waiter: who can take one order at a time\n",
    "- Customers(Table) think about the order for 4 seconds\n",
    "- Waiter takes down order in 1 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is thinking about the order\n",
      "A has given it's order to John Doe\n",
      "B is thinking about the order\n",
      "B has given it's order to John Doe\n",
      "C is thinking about the order\n",
      "C has given it's order to John Doe\n",
      "D is thinking about the order\n",
      "D has given it's order to John Doe\n",
      "E is thinking about the order\n",
      "E has given it's order to John Doe\n",
      "taking all orders took 25 seconds\n"
     ]
    }
   ],
   "source": [
    "# sequential example\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class Waiter(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def take_order(self):\n",
    "        time.sleep(1)\n",
    "\n",
    "class Table(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.order_taken = False\n",
    "        self.name = name\n",
    "    \n",
    "    def give_order(self, waiter):\n",
    "        print(f\"{self.name} is thinking about the order\")\n",
    "        time.sleep(4)\n",
    "        self.order_taken = True\n",
    "        waiter.take_order()\n",
    "        print(f\"{self.name} has given it's order to {waiter.name}\")\n",
    "\n",
    "tables = [Table(\"A\"), Table(\"B\"), Table(\"C\"), Table(\"D\"), Table(\"E\")]\n",
    "waiter = Waiter(\"John Doe\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "for table in tables:\n",
    "    table.give_order(waiter)\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"taking all orders took {elapsed_time.seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The free lunch is over\n",
    "- approaching the physical limits of Moore's Law, processors not getting faster\n",
    "- way forward is to have MORE processors, not faster processors\n",
    "- Ahmdal's Law says that gains are bounded by how much of the program has to run in a sequential manner. (The more you can can parallelize, the faster you can go)\n",
    "- allows for scaling 'horizontally'\n",
    "- explains the sudden surge of interest in Erlang, Scala, Clojure and FP\n",
    "\n",
    "[The free lunch is over](http://www.gotw.ca/publications/concurrency-ddj.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "- moore's law says that the number of transistors on an IC doubles every two years, which directly correlates with processor speed\n",
    "- corresponds to scaling the system vertically, i.e increasing the throughput of a system\n",
    "- on the other hand scaling horizontally means adding more instances of the same system to increase throughput\n",
    "- these are the languages which offer better constructs for concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "- The phenomena we're talking about are everpresent around us, though they're of particular interest in CS.\n",
    "- If all this talk of processors, and web scale confused you, let's take a more grounded example.\n",
    "- I find it helpful when learning new things to have the same thing presented to me in different ways\n",
    "- So let's make our restaurant example concurrent. Instead of blocking the waiter while the customer is thinking, the waiter goes to whichever table is ready to order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is thinking about the order\n",
      "B is thinking about the order\n",
      "C is thinking about the order\n",
      "D is thinking about the order\n",
      "E is thinking about the order\n",
      "B has given it's order to John Doe\n",
      "A has given it's order to John Doe\n",
      "E has given it's order to John Doe\n",
      "D has given it's order to John Doe\n",
      "C has given it's order to John Doe\n",
      "took 9 seconds\n"
     ]
    }
   ],
   "source": [
    "# multithreaded example\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class Waiter(object):\n",
    "    lock = threading.Lock()\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def take_order(self):\n",
    "        with self.lock:  # TODO: queue example if adequate time\n",
    "        # waiter can take only one order at a time.\n",
    "        # Sleep does not block the entire Python process\n",
    "        # here context switch will happen to another thread\n",
    "        # so we have to lock so that the waiter isn't\n",
    "        # allowed to be at two tables at once\n",
    "            time.sleep(1)\n",
    "\n",
    "class Table(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.order_taken = False\n",
    "        self.name = name\n",
    "    \n",
    "    def give_order(self, waiter):\n",
    "        print(f\"{self.name} is thinking about the order\")\n",
    "        time.sleep(4)\n",
    "        self.order_taken = True\n",
    "        waiter.take_order()\n",
    "        print(f\"{self.name} has given it's order to {waiter.name}\")\n",
    "\n",
    "tables = [Table(\"A\"), Table(\"B\"), Table(\"C\"), Table(\"D\"), Table(\"E\")]\n",
    "waiter = Waiter(\"John Doe\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "threads = []\n",
    "for table in tables:\n",
    "    thread = threading.Thread(target=table.give_order, args=(waiter,))\n",
    "    # all tables start thinking about ordering\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()  # wait for all threads to finish\n",
    "    # otherwise execution will continue without waiting for threads\n",
    "    # to end, and we'll get elapsed time as 0.\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"took {elapsed_time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency is not parallelism\n",
    "Concurrency is not parallelism\n",
    "\n",
    "concurrency: Dealing with a lot at once, property of the solution (code)\n",
    "parallelism: Doing a lot at once, property of the runtimeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you should care\n",
    "- Knowing this distinction allows you to pick the right tool for the right job\n",
    "- Apparently, Python is bad for programming concurrent programs because only one python thread can run at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!, I am thread Thread-1\n",
      "Hello, World!, I am thread Thread-2\n",
      "Hello, World!, I am thread Thread-3\n",
      "Hello, World!, I am thread Thread-4\n",
      "Hello, World!, I am thread Thread-5\n",
      "elapsed time 5 s\n"
     ]
    }
   ],
   "source": [
    "# sequential_client.py\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "for i in range(5):\n",
    "    print(requests.get(\"http://127.0.0.1:5000\").text)\n",
    "end_time = datetime.now()\n",
    "print(f\"elapsed time {(end_time - start_time).seconds} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threaded_client.py\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "def make_request():\n",
    "    print(requests.get(\"http://127.0.0.1:5000\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes\n",
    "- Here, orders are being processed concurrently, even though there's no parallelism.\n",
    "- We can add parallelism by adding another waiter\n",
    "- Let's talk about python\n",
    "- We used python threads in the second approach, let's talk more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading in python\n",
    "- User-level threads: Threads that we can actively create, run, and kill for all of our various tasks (Python)\n",
    "- Kernel-level threads: Very low-level threads acting on behalf of the operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Advantages__\n",
    "- Multiple threads are excellent for speeding up blocking I/O bound programs\n",
    "- Schedulted preemptively - don't have to write any extra code for it\n",
    "- They are lightweight in terms of memory footprint when compared to processes\n",
    "- Threads share resources, and thus communication between them is easier\n",
    "__Disdavantages__\n",
    "- CPython threads are hamstrung by the limitations of the global interpreter lock, which means that python threads can't make use of multiple CPU cores\n",
    "(GIL)\n",
    "- While communication between threads may be easier, you must be very careful not to implement code that is subject to race conditions. It is a comparitively quite difficult to get this right.\n",
    "- Hard to test, hard to spot bugs, hard to reproduce bugs\n",
    "- It's computationally expensive to switch context between multiple threads. By adding multiple threads, you could see a degradation in your program's overall performance if not used correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "- Preemptive scheduling is a double edged sword like most things.\n",
    "- Easy to execute threads, but since you can't make any assumptions about when the task switch might happen, you have to guard portions of your code that access a shared resource(The critical section)\n",
    "- This access is synchronized via locks, but this is notoriously hard to get right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dreaded GIL\n",
    "- Python has one global lock, on the entire interpreter instead of thousands of granular locks everywhere else.\n",
    "- Locking and unlocking is not free, previous attempts at removing the GIL has resulted in severe degradation of the performance of a single thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Python threads have to acquire a mutex lock on the interpreter to execute.\n",
    "- What that essentially means is that only one thread can run in a python process at one time, utilizing only one core at a time.\n",
    "- It is necessary because python interpreters internal datastructures aren't thread safe\n",
    "- (Book says memory management isn't thread safe - same thing I think). Previous attempts at\n",
    "- removing the GIL have been unsuccessful as the overhead of locking degraded performance considerably. So CPU bound tasks are going to be much slower, were the gil removed. The model of having GIL with parallelism deferred to the OS via the\n",
    "- For utilizing multiple cores, multiprocessing module works well.\n",
    "- Note that the GIL is present in only the default implementation - CPython, and doesn't exist in runtimes who support parallel threads like Jython, IronPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use\n",
    "- I/O bound processes\n",
    "- Blocking I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's approach to parallelism - Don't DIY\n",
    "- To make use of all cores, python prescribes multiprocessing.\n",
    "- Utilizing multiple cores is left up to the OS (since processes are scheduled by the OS).\n",
    "\n",
    "__Advantages__:\n",
    "- They are better than multiple threads at handling CPU-intensive tasks\n",
    "- We can sidestep the limitations of the GIL by spawning multiple processes\n",
    "- Crashing processes will not kill our entire program(workers model - give example of Gunicorn - If you've run any python webapp/service in production, you might have heard of it. When your program is leaking memory, just reload it. No shared state - you can do it)\n",
    "\n",
    "__Disadvantages__:\n",
    "- No shared memory between processes - have to implement some form of IPC, which is much more resource heavy on the computer.\n",
    "- slower context switches.\n",
    "- more startup cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "- Let's see this in action.\n",
    "- Starting from our non blocking IO example\n",
    "(show non blocking IO multi processing example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async\n",
    "- cooperative scheduling - need explicit code to cause task switch.\n",
    "- Typically single threaded\n",
    "- Since you control when task switches occur, you don't need locks for synchronization.\n",
    "__Advantages__:\n",
    "- very low cost, since no context switch. Incidentally they're cheaper than function calls. Cheaper switching mechanism among all techniques. People prefer this model to locking, \n",
    "- No cost of synchronization = less CPU consumption. Async servers > threaded servers. You can run many many more async tasks than threads.\n",
    "__disadvantages__:\n",
    "- Need code that gives up control\n",
    "- Need an event loop\n",
    "- Everything has to be non-blocking.\n",
    "- Need to learn a lot of new things - new syntax, new libraries(aysnc versions), eventloops, futures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back at the restaurant\n",
    "An order task involves:\n",
    "- Choosing items\n",
    "- placing your order with the waiter\n",
    "- waiting for your order\n",
    "- Digging inüç¥\n",
    "- Waiter goes around the tables, and finally to the kitchen (scheduler/eventloop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is thinking about the order\n",
      "B is thinking about the order\n",
      "C is thinking about the order\n",
      "D is thinking about the order\n",
      "E is thinking about the order\n",
      "A has given it's order to John Doe\n",
      "B has given it's order to John Doe\n",
      "C has given it's order to John Doe\n",
      "D has given it's order to John Doe\n",
      "E has given it's order to John Doe\n",
      "taking all orders took 9 seconds\n"
     ]
    }
   ],
   "source": [
    "# async example\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class Waiter(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def take_order(self):\n",
    "        # blocks thread, hence blocking the entire event loop.\n",
    "        time.sleep(1)\n",
    "\n",
    "class Table(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.order_taken = False\n",
    "        self.name = name\n",
    "\n",
    "    # async keyword defines a coroutine\n",
    "    async def give_order(self, waiter):\n",
    "        print(f\"{self.name} is thinking about the order\")\n",
    "        await asyncio.sleep(4)  # This is how we give up control\n",
    "        # Execution will resume after 4 s.\n",
    "        self.order_taken = True\n",
    "        waiter.take_order()\n",
    "        print(f\"{self.name} has given it's order to {waiter.name}\")\n",
    "\n",
    "tables = [Table(\"A\"), Table(\"B\"), Table(\"C\"), Table(\"D\"), Table(\"E\")]\n",
    "waiter = Waiter(\"John Doe\")\n",
    "\n",
    "eventloop = asyncio.get_event_loop()\n",
    "start_time = datetime.now()\n",
    "tasks = [eventloop.create_task(table.give_order(waiter))\n",
    "         for table in tables]\n",
    "await asyncio.wait(tasks)\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"taking all orders took {elapsed_time.seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "- when you're choosing what to order, the waiter need not be blocked.\n",
    "- similarly, your program doesn't have to block on a slow network call, your program can be doing other things. This is the foundation of AJAX\n",
    "- Unicode supports emojis now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
